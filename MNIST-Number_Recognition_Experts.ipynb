{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "----------------\n",
    "This is a modified version of Tensorflow's Tutorial: [Deep MNIST for Experts ](https://www.tensorflow.org/get_started/mnist/pros).\n",
    "\n",
    "For convenience, here we use built-in functions of TensorFlow directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "----------------\n",
    "* Training data is from **THE MNIST DATABASE of handwritten digits** (image + label = 60000 data): http://yann.lecun.com/exdb/mnist/\n",
    "  * [train-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz):  training set images (9912422 bytes) \n",
    "  * [train-labels-idx1-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz):  training set labels (28881 bytes)\n",
    "  \n",
    "\n",
    "* Testing Data:\n",
    "  * [t10k-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz): test set images (1648877 bytes) \n",
    "  * [t10k-labels-idx1-ubyte.gz](http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz): test set labels (4542 bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "----------------\n",
    "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998. [[on-line version]](http://yann.lecun.com/exdb/publis/index.html#lecun-98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./MNIST-data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9027\n",
      "Data Preprocessing Elapsed Time: 1.59175705909729\n",
      "TensorFlow         Elapsed Time: 2.4740309715270996\n",
      "Training           Elapsed Time: 0.9172837734222412\n",
      "Total              Elapsed Time: 4.065868854522705\n"
     ]
    }
   ],
   "source": [
    "import gzip, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def read_data(train_data_path, train_label_path,\n",
    "              test_data_path, test_label_path):\n",
    "    train_data, train_label = None, None\n",
    "    with gzip.open(train_data_path, 'rb') as f: # train data\n",
    "        train_data = np.reshape(np.frombuffer(f.read(), dtype=np.uint8, offset=16), (60000, -1))\n",
    "    with gzip.open(train_label_path, 'rb') as f: # train label\n",
    "        train_label = np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "    \n",
    "    test_data, test_label = None, None\n",
    "    with gzip.open(test_data_path, 'rb') as f: # test data\n",
    "        test_data = np.reshape(np.frombuffer(f.read(), dtype=np.uint8, offset=16), (10000, -1))\n",
    "    with gzip.open(test_label_path, 'rb') as f: # test label\n",
    "        test_label = np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "    return train_data, train_label, test_data, gen_one_hot(np.array(test_label))\n",
    "\n",
    "def gen_one_hot(labels, _class=10):\n",
    "    one_hot_labels = np.zeros(shape=(np.array(labels).shape[0], _class), dtype=np.int)\n",
    "    for index, label in enumerate(labels):\n",
    "        one_hot_labels[index] = [1 if i==label else 0 for i in range(_class)]\n",
    "    return one_hot_labels\n",
    "\n",
    "def split_data(train_images, train_labels, num_img=60000, num_valid=5000, nb_classes=10):\n",
    "    rand_list = np.random.choice(num_img, num_valid, replace=False)\n",
    "    X_train = [train_images[i] for i in np.setdiff1d(np.arange(num_img), rand_list)]\n",
    "    X_valid = [train_images[i] for i in rand_list]\n",
    "    Y_train = [train_labels[i] for i in np.setdiff1d(np.arange(num_img), rand_list)]\n",
    "    Y_valid = [train_labels[i] for i in rand_list]\n",
    "    Y_train = np.array(gen_one_hot(np.array(Y_train), nb_classes))\n",
    "    Y_valid = np.array(gen_one_hot(np.array(Y_valid), nb_classes))\n",
    "    return np.array(X_train), np.array(X_valid), Y_train, Y_valid\n",
    "\n",
    "\n",
    "# data preprocessing part\n",
    "# ------------------------------------------------------\n",
    "start = time.time()\n",
    "\n",
    "data_start = time.time()\n",
    "\n",
    "prefix = './MNIST-DATA/'\n",
    "train_images, train_labels, test_imgs, test_labels = read_data(\n",
    "    prefix+'train-images-idx3-ubyte.gz',\n",
    "    prefix+'train-labels-idx1-ubyte.gz',\n",
    "    prefix+'t10k-images-idx3-ubyte.gz',\n",
    "    prefix+'t10k-labels-idx1-ubyte.gz')\n",
    "X_train, X_valid, Y_train, Y_valid = split_data(train_images, train_labels, num_valid=0)\n",
    "\n",
    "data_end = time.time()\n",
    "\n",
    "\n",
    "# tensorflow calculation part\n",
    "# ------------------------------------------------------\n",
    "tf_start = time.time()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=tf.matmul(x, W) + b, labels=y_)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) # Gradient Descent\n",
    "# train_step = tf.train.AdamOptimizer().minimize(cross_entropy) # Adam\n",
    "\n",
    "epoch = 1000\n",
    "rnd = [np.random.choice(X_train.shape[0], 100, replace=False) for i in range(epoch)]\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "train_start = time.time()\n",
    "for i in range(epoch):\n",
    "    batch_xs, batch_ys = np.array([X_train[r] for r in rnd[i]]) , np.array([Y_train[r] for r in rnd[i]])\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "train_end = time.time()\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test Accuracy: ' + str(sess.run(accuracy, feed_dict={x: test_imgs, y_: test_labels})))\n",
    "\n",
    "tf_end = time.time()\n",
    "\n",
    "end = time.time()\n",
    "print('Data Preprocessing Elapsed Time: ' + str(data_end-data_start))\n",
    "print('TensorFlow         Elapsed Time: ' + str(tf_end-tf_start))\n",
    "print('Training           Elapsed Time: ' + str(train_end-train_start))\n",
    "print('Total              Elapsed Time: ' + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Gradient Descent Model (read data from tensorflow mnist dataset)\n",
    "---\n",
    "* I used python built-in module **time** to estimate elapsed time approximately:  \n",
    "> ```python\n",
    "> start = time.time()\n",
    "> # ...\n",
    "> # function to be estimated\n",
    "> # ...\n",
    "> end = time.time()\n",
    "> print(end - start)\n",
    "> \n",
    "> ```\n",
    "* Loss Function (cross-entropy ):\n",
    "> **$ H_{y'}(y) = -\\sum_i y'_i \\log(y_i) $**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST-DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST-DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST-DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST-DATA/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Test Accuracy: 0.8761\n",
      "Data Preprocessing Elapsed Time: 0.5083792209625244\n",
      "TensorFlow         Elapsed Time: 1.0171470642089844\n",
      "Training           Elapsed Time: 0.8346719741821289\n",
      "Total              Elapsed Time: 1.5256309509277344\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# data preprocessing part\n",
    "# ------------------------------------------------------\n",
    "start = time.time()\n",
    "\n",
    "data_start = time.time()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST-DATA/\", one_hot=True)\n",
    "\n",
    "data_end = time.time()\n",
    "\n",
    "\n",
    "# tensorflow calculation part\n",
    "# ------------------------------------------------------\n",
    "tf_start = time.time()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=tf.matmul(x, W) + b, labels=y_)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) # Gradient Descent\n",
    "# train_step = tf.train.AdamOptimizer().minimize(cross_entropy) # Adam\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "train_start = time.time()\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "train_end = time.time()\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('\\nTest Accuracy: ' + str(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "\n",
    "tf_end = time.time()\n",
    "\n",
    "end = time.time()\n",
    "print('Data Preprocessing Elapsed Time: ' + str(data_end-data_start))\n",
    "print('TensorFlow         Elapsed Time: ' + str(tf_end-tf_start))\n",
    "print('Training           Elapsed Time: ' + str(train_end-train_start))\n",
    "print('Total              Elapsed Time: ' + str(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
